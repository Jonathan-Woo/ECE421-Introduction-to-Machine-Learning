{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "jupytext": {
      "formats": "ipynb,md:myst"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtWX4x9DCF5_"
      },
      "source": [
        "# JAX\n",
        "\n",
        "**JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY8mDvEvCGqk"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D2KXNfJXL5N"
      },
      "source": [
        "Make sure JAX is using the GPU. First, remember to turn on the GPU runtime in Colab: \n",
        "\n",
        "**Click on Runtime -> Click on \"runtime type\" -> Choose GPU**\n",
        "\n",
        "then:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY0LfOk6PXTq",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "import jax\n",
        "jax.config.update('jax_platform_name', 'gpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHHrna5MW6J8"
      },
      "source": [
        "JAX provides a drop-in replacement for many of the essential NumPY functionalities. These replacements would run faster because they are parallelized and thus run efficiently on the GPU. Let's see an example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ89jHCYfhpg"
      },
      "source": [
        "## Multiplying Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpy1dSgNqCP4"
      },
      "source": [
        "We'll be generating random data in the following examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0nseKZNqOoH",
        "outputId": "70d102b3-6c74-4b5b-f4c8-b2a9d476a5de"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "key, subkey = random.split(key)\n",
        "x = random.normal(subkey, (10,))\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.38812608 -0.04487164 -2.0427258   0.07932311  0.33349916  0.7959976\n",
            " -1.4411978  -1.6929979  -0.37369204 -1.5401139 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s6KpHrAYI6y"
      },
      "source": [
        "**Note**: As you saw we used the `PRNGKey()` to generate our random variables. One big difference between NumPy and JAX is how you generate random numbers. In JAX:\n",
        "\n",
        "> We split the PRNG to get usable subkeys every time we need a new pseudorandom number. We **propagate the key** and **use the new subkey** whenever we need new a random number.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "668Ke8lwvcH6"
      },
      "source": [
        "For more details, see \n",
        "[Common Gotchas in JAX](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXn8GUl6CG5N",
        "outputId": "effd2f64-aad6-49c1-e060-aa19f3b95c3e"
      },
      "source": [
        "size = 3000\n",
        "key, subkey = random.split(key)\n",
        "x = random.normal(subkey, (size, size), dtype=jnp.float32)\n",
        "%timeit jnp.dot(x, x.T).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 83.64 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 28.5 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x9J0WD_ZMIf"
      },
      "source": [
        "The same operation using NumPy takes 3 times longer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPl0MuwYrM7t",
        "outputId": "067f836c-032e-4388-8a9d-af791b642fc1"
      },
      "source": [
        "import numpy as np\n",
        "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
        "%timeit jnp.dot(x, x.T).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 92.2 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANGInHfKZUtp"
      },
      "source": [
        "It is important to know where your data lives. When using JAX with GPU, the data lives on VRAM (the graphics card memory) and not RAM. If you want to transfer a JAX array between RAM and VRAM, yo use the `device_put(device=None)` function that by default commits (copies a \"deep\" copy of) your data to your default memory (here, VRAM because we chose to work on GPU before).   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj7M7zyRskF0",
        "outputId": "ecf6d95e-5683-40eb-be3a-1499643fc51d"
      },
      "source": [
        "from jax import device_put\n",
        "\n",
        "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
        "x = device_put(x) # this puts the data in jax.devices()[0] which for us, is the (first) GPU's VRAM. If you don't use GPU, then this will be in RAM.\n",
        "%timeit jnp.dot(x, x.T).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 24.3 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clO9djnen8qi"
      },
      "source": [
        "The output of `~jax.device_put` still acts like an NDArray (a NumPy array), but it only copies values back to the CPU when they're needed for printing, plotting, saving to disk, branching, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOzp0P_GoJhb"
      },
      "source": [
        "JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numerical code. For now, there are three main ones:\n",
        "\n",
        " - {func}`~jax.jit`, for speeding up your code\n",
        " - {func}`~jax.grad`, for taking derivatives\n",
        " - {func}`~jax.vmap`, for automatic vectorization or batching.\n",
        "\n",
        "Let's go over these, one-by-one. We'll also end up composing these in interesting ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTTrTbWvgLUK"
      },
      "source": [
        "## Using `jax.jit` to speed up functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETIftvbm8gqT"
      },
      "source": [
        "You may have heard that python in \"interpreted\", i.e., it is run one line at a time. This makes it very convenient to use, especially with data science and machine learning workloads where rapid iteration is highly desirable. But this comes at a performance cost. That is why most performant ML libraries (like NumPy) have wrappers in lower-level compiled languages (like C). Just-in-time compilation, brings the best of both worlds:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc7SYygo7-qt"
      },
      "source": [
        "\"In computing, just-in-time (JIT) compilation (also dynamic translation or run-time compilations) is a way of executing computer code that involves compilation during execution of a program (at run time) rather than before execution...JIT compilation is a combination of the two traditional approaches to translation to machine code (ahead-of-time compilation (AOT), and interpretation)\"&mdash;[JIT on Wikipedia](https://en.wikipedia.org/wiki/Just-in-time_compilation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQsPzZtxMhxB"
      },
      "source": [
        "JIT compilation is now pretty common in numerical computing. For instance, the computing language Julia is also JIT compiled by-default. Besides providing a performant GPU-optimized NumPy alternative, JAX provides a convenient way to perform JIT compilation to speed up computations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVdsPt91ToMZ"
      },
      "source": [
        "For example, consider the Rectified Linear Unit function, or ReLU: $$\\text{relu}(x) = \\max\\{x, 0\\} = (x)^{+}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLGdCtFKFLOR",
        "outputId": "dfd18661-5a39-4872-baf6-be059c4a4ebc"
      },
      "source": [
        "def relu(x):\n",
        "    return jnp.where(x > 0, x, 0)\n",
        "\n",
        "key, subkey = random.split(key)\n",
        "x = random.normal(subkey, (1000000,))\n",
        "%timeit relu(x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 143.12 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 1.19 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_V8SruVHrD_"
      },
      "source": [
        "We can speed it up with `jit`, which will jit-compile the first time `relu` is called and will be cached thereafter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh4w_3NpFYTp",
        "outputId": "9d4862f7-7256-4149-d64d-8f74b6bf0a1b"
      },
      "source": [
        "relu_jit = jit(relu)\n",
        "%timeit relu_jit(x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 660.66 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 130 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEHgQnfQVclo"
      },
      "source": [
        "That is, we gained a 10 times speedup!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxpBc4WmfsEU"
      },
      "source": [
        "## Taking derivatives with `jax.grad`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFYNQNckV12y"
      },
      "source": [
        "Perhaps the most important functionality that JAX provides us is the automatic differentiation (AD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfOoOKQiXWmL"
      },
      "source": [
        "Consider a sum of logits:\n",
        "$$ f(\\mathbf{x}) = \\sum_i \\sigma(x_i) = \\sum_i  \\frac{1}{1 + e^{-x_i}}$$\n",
        "\n",
        "We want to calculate $\\nabla_\\mathbf{x}f$. Let's derive it analytically first:\n",
        "\n",
        "$$\n",
        "\\frac{\\mathrm{\\partial}}{\\mathrm{\\partial} x_i} f(\\mathbf{x})=\\frac{e^{x} \\cdot\\left(1+e^{x_i}\\right)-e^{x_i} \\cdot e^{x_i}}{\\left(1+e^{x_i}\\right)^{2}}=\\frac{e^{x_i}}{\\left(1+e^{x_i}\\right)^{2}}= \\sigma(x_i)\\left(1- \\sigma(x_i)\\right)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMAgNJaMJwPD"
      },
      "source": [
        "def sum_logistic(x):\n",
        "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtNs881Ohioc"
      },
      "source": [
        "So, provided you use JAX primitives, JAX automatically stores the gradients and values in the computation graph defined by your function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT_dxAD5Ztay",
        "outputId": "d15d1ff8-7076-4d19-90d6-e1fb635e5acb"
      },
      "source": [
        "x_small = jnp.arange(3.)\n",
        "derivative_fn = grad(sum_logistic)\n",
        "print(derivative_fn(x_small))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25       0.19661194 0.10499357]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU1Zmi2VZvk5"
      },
      "source": [
        "Let's verify with our analytically derived expression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFhAJAkXZ2UK",
        "outputId": "25058ac7-6a18-48aa-bd4e-e7cd4f2738e3"
      },
      "source": [
        "def grad_sum_logistic(x):\n",
        "    return jnp.exp(x) / jnp.power(1.0 + jnp.exp(x), 2)\n",
        "\n",
        "grad_sum_logistic(x_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0.25      , 0.19661194, 0.10499358], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfs-3FYjapqk"
      },
      "source": [
        "We can go further, and take the Hessian as well. But there is a technical issue: gradient is a vector, and although in notation we write the Hessian as a gradient of the gradient, this is not technically true, since only scalers have gradients.  In fact, when we take the Hessian, we take a gradient of every element of the gradient. \n",
        "\n",
        "One solution is to add an additional vector $\\mathbf{v}$ and do a dot product with the gradient, so that we get a scaler again. Now we can again use the `grad` function: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-JxbiNyhxEW"
      },
      "source": [
        "def hessian_fn(x, v):\n",
        "    intermediate_fn = lambda x : jnp.vdot(derivative_fn(x), v)\n",
        "    derivative_intermediate_fn = grad(intermediate_fn)\n",
        "    return derivative_intermediate_fn(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DL-kZa6hvxF"
      },
      "source": [
        "Now if we let $\\mathbf{v}$  be the unit vectors of this 3-dimensional space, we can recover the Hessian of our function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTSl12tPhzgU",
        "outputId": "cfaff835-aa79-4973-ec9f-26399f654c98"
      },
      "source": [
        "print(hessian_fn(x_small, jnp.array([1, 0, 0])))\n",
        "print(hessian_fn(x_small, jnp.array([0, 1, 0])))\n",
        "print(hessian_fn(x_small, jnp.array([0, 0, 1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0. -0. -0.]\n",
            "[-0.         -0.09085774 -0.        ]\n",
            "[-0.         -0.         -0.07996248]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhne9N5xixJn"
      },
      "source": [
        "But this is cumbersome. We can do this in one go by recognizing that if we stack the $\\mathbf{v}$'s above together, they make an identity matrix of dimensions 3. Using the `stack` function and Python's list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAr6G0PDi8Hm",
        "outputId": "d0734246-0376-4b8b-c0a6-964e8df578ba"
      },
      "source": [
        "def hessian_fn_stacked(x):\n",
        "  return jnp.stack([hessian_fn(x_small, unit_vector) for unit_vector in jnp.identity(3)])\n",
        "\n",
        "hessian_fn_stacked(x_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-0.        , -0.        , -0.        ],\n",
              "             [-0.        , -0.09085774, -0.        ],\n",
              "             [-0.        , -0.        , -0.07996248]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lS0nIWCjV2J"
      },
      "source": [
        "This is an example of a function that could really use **vectorization**. Fortunately, JAX provides a very convenient way to vectorize functions (aka maps):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4kWJ4PEiWEQ",
        "outputId": "a6861e91-4276-4a8b-a884-37567d09f735"
      },
      "source": [
        "def hessian_fn_vmap(x):\n",
        "    vectorized_hessian_fn = vmap(lambda v: hessian_fn(x, v))\n",
        "    return vectorized_hessian_fn(jnp.identity(3))\n",
        "\n",
        "hessian_fn_vmap(x_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-0.        , -0.        , -0.        ],\n",
              "             [-0.        , -0.09085774, -0.        ],\n",
              "             [-0.        , -0.        , -0.07996248]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hGQwgd6mEdj"
      },
      "source": [
        "We could have gotten our Hessian in a much more compact way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsKoEVPlmQpU",
        "outputId": "bc784e52-0c91-4009-eb4b-9d6679196b12"
      },
      "source": [
        "vmap(lambda v: hessian_fn(x_small, v))(jnp.identity(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-0.        , -0.        , -0.        ],\n",
              "             [-0.        , -0.09085774, -0.        ],\n",
              "             [-0.        , -0.        , -0.07996248]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsuWQt9ULk2w"
      },
      "source": [
        "## Taking Jacobians with `jax.jacfwd` and `jax.jacbwd`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDsK5wViFy4X"
      },
      "source": [
        "We can write an more elegant (and performant) code if we used JAX's Jacobian function which extends taking gradient to vector-valued functions:\n",
        "\n",
        "If we start with a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, then at a point $x \\in \\mathbb{R}^n$ we expect to get the shapes\n",
        "\n",
        "$f(x) \\in \\mathbb{R}^m$, the value of $f$ at $x$,\n",
        "\n",
        "$\\partial f(x) \\in \\mathbb{R}^{m \\times n}$, the Jacobian matrix at $x$,\n",
        "\n",
        "$\\partial^2 f(x) \\in \\mathbb{R}^{m \\times n \\times n}$, the Hessian at $x$.\n",
        "\n",
        "In our example above, $m=1$ and $n=3$.\n",
        "\n",
        "JAX has two implementation of the Jacobian, `jax.jacfwd` and `jax.jacrev`, which correspond to [forward-mode and backward-mode](https://en.wikipedia.org/wiki/Automatic_differentiation) automatic differentiation, respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cqr0zc_FXa9",
        "outputId": "6076d3bb-54d7-4ca5-a098-2fd0363c7be8"
      },
      "source": [
        "print(jax.jacfwd(jax.grad(sum_logistic))(x_small))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.         -0.         -0.        ]\n",
            " [-0.         -0.09085774 -0.        ]\n",
            " [-0.         -0.         -0.07996248]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2cWwQBpFXIZ"
      },
      "source": [
        "Since the gradient is the Jacobian of a scaler ($m=1$) , we could have used `jacrev` or `jacfwd` to the same effect. Also we can freely compose these. So all these forms are equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUf6HhuCKnK0",
        "outputId": "7a70d863-b75a-4029-eefc-18e486ea5e00"
      },
      "source": [
        "print(jax.jacfwd(jax.jacrev(sum_logistic))(x_small))\n",
        "print(jax.jacrev(jax.jacfwd(sum_logistic))(x_small))\n",
        "print(jax.jacfwd(jax.jacfwd(sum_logistic))(x_small))\n",
        "print(jax.jacrev(jax.jacrev(sum_logistic))(x_small))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.         -0.         -0.        ]\n",
            " [-0.         -0.09085774 -0.        ]\n",
            " [-0.         -0.         -0.07996248]]\n",
            "[[-0.         -0.         -0.        ]\n",
            " [-0.         -0.09085774 -0.        ]\n",
            " [-0.         -0.         -0.07996248]]\n",
            "[[ 0.          0.          0.        ]\n",
            " [ 0.         -0.09085774  0.        ]\n",
            " [ 0.          0.         -0.07996248]]\n",
            "[[-0.         -0.         -0.        ]\n",
            " [-0.         -0.09085774 -0.        ]\n",
            " [-0.         -0.         -0.07996248]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TELSdz2K8PP"
      },
      "source": [
        "The only difference then, is performance; which is an important concern, espeically in neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT71Xf2oJoM3",
        "outputId": "32599189-36cd-4c69-f90a-16979e60a387"
      },
      "source": [
        "%timeit jax.jacfwd(jax.jacrev(sum_logistic))(x_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 15.1 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvA2keVfKSjN",
        "outputId": "3ce101e9-2a1c-453e-89f5-76fbc014ee01"
      },
      "source": [
        "%timeit jax.jacrev(jax.jacfwd(sum_logistic))(x_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 15.5 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkQkAzTSLMwJ"
      },
      "source": [
        "You can read in more detail about the details of how these are different in the JAX's [autodiff cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html) but all we need for now is:\n",
        "\n",
        "> `jacfwd` is more efficient for “tall” Jacobian matrices, while `jacrev` is more efficient for “wide” Jacobian matrices. For matrices that are near-square, `jacfwd` probably has an edge over `jacrev`.\n",
        "\n",
        "Since the gradient is a 'tall' matrix, we use `jacfwd`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOfP0KeZKiRZ",
        "outputId": "bc3aff84-4745-4d2d-e710-5e0830fb0521"
      },
      "source": [
        "%timeit jax.jacfwd(jax.grad(sum_logistic))(x_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 11.6 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgtldQnTMHcv"
      },
      "source": [
        "We can make the code more performant by compiling it just in time (*jitting* it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URrp1ebVMRf9",
        "outputId": "e347d8b6-42fe-4ab0-edfa-f83232713ed7"
      },
      "source": [
        "hessian_jitted  = jit(jax.jacfwd(jit(jax.grad(sum_logistic))))\n",
        "hessian_jitted(x_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-0.        , -0.        , -0.        ],\n",
              "             [-0.        , -0.09085774, -0.        ],\n",
              "             [-0.        , -0.        , -0.07996248]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLICcYYBM1h-",
        "outputId": "fa46e723-a0e2-4446-cb64-f6031fb0aa31"
      },
      "source": [
        "%timeit hessian_jitted(x_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.72 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 111 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYyGUn_CNYo3"
      },
      "source": [
        "Let's compare this to our slowest implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ki15rxNbsM",
        "outputId": "076124ea-2194-4af0-a3e6-5f1b52d4fb59"
      },
      "source": [
        "%%timeit\n",
        "hessian_fn(x_small, jnp.array([1, 0, 0]))\n",
        "hessian_fn(x_small, jnp.array([0, 1, 0]))\n",
        "hessian_fn(x_small, jnp.array([0, 0, 1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 47.1 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cjgbsz2OIJu"
      },
      "source": [
        "That's nearly 500 times faster!\n",
        "\n",
        "It's important to note that in machine learning, writing performant code is not a nicety; it's what enables training models on millions of samples in days rather than years. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI4nPsGafxbL"
      },
      "source": [
        "## Auto-vectorization with `jax.vmap`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Be45XzWQiKK"
      },
      "source": [
        "Vectorization basically means to apply the same function (*map*) to a vector of values all at once, instead of one value at a time (sequentially). \n",
        "\n",
        "This is an important concept in parallelized processing of data which allows us to gain significant speedups on massively parallelized hardware such as GPUs; or distributed clusters. \n",
        "\n",
        "Python provides us with a simple `map` that applies a function onto a iterator (such as a list comprehension) or a generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hzehO_yS0Fy",
        "outputId": "ca200fd1-078e-4bfa-82c3-22f384a4ff28"
      },
      "source": [
        "map(lambda x: x*2, range(1, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<map at 0x7f2835a828d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrg8Z2FETULm"
      },
      "source": [
        "For example, imagine we want to multiply matrix of weights `w` with `samples`. We can do this reasonably fast with NumPy or JAX equivalent function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w0Gpsn8WYYj",
        "outputId": "7a969d96-5cdf-4a7d-f940-b9f78d1d0727"
      },
      "source": [
        "key, *subkeys = random.split(key, 3) # split the key into 3 keys, we propagate one and use the other two\n",
        "w = random.normal(subkeys[0], (150, 100))\n",
        "samples = random.normal(subkeys[1], (10, 100))\n",
        "\n",
        "%timeit jnp.dot(w, samples.T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 200.61 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 414 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1Kc-nQFUt3I"
      },
      "source": [
        "def apply_matrix(v):\n",
        "    return jnp.dot(w, v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWVc9BsZv0Ki",
        "outputId": "6f31b3e0-8d16-4cc6-fc11-1079a3c093ee"
      },
      "source": [
        "def naively_batched_apply_matrix(v_batched):\n",
        "    return jnp.stack([apply_matrix(v) for v in v_batched])\n",
        "\n",
        "print('Naively batched')\n",
        "%timeit naively_batched_apply_matrix(samples).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naively batched\n",
            "The slowest run took 33.59 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "100 loops, best of 5: 5.7 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipei6l8nvrzH",
        "outputId": "423f34a2-715f-4a9a-f35d-051ebef9a164"
      },
      "source": [
        "@jit\n",
        "def batched_apply_matrix(v_batched):\n",
        "    return jnp.dot(v_batched, w.T)\n",
        "\n",
        "print('Manually batched')\n",
        "%timeit batched_apply_matrix(samples).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually batched\n",
            "The slowest run took 698.96 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 116 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Oeknf5vuCl",
        "outputId": "43f0456e-d65b-48b8-94de-797df859d8cf"
      },
      "source": [
        "@jit\n",
        "def vmap_batched_apply_matrix(v_batched):\n",
        "    return vmap(apply_matrix)(v_batched)\n",
        "\n",
        "print('Auto-vectorized with vmap')\n",
        "%timeit vmap_batched_apply_matrix(samples).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-vectorized with vmap\n",
            "The slowest run took 642.32 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 135 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru9UkjlzYxKe"
      },
      "source": [
        "# K-Means in JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFkrnyXBY0mL"
      },
      "source": [
        "Consider the following randomly generated dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phNmeM2MZYoK",
        "outputId": "c5bb16d4-ecaa-4ded-f145-63b573d25460"
      },
      "source": [
        "key, *subkeys = random.split(key, 4)\n",
        "points = jnp.concatenate([\n",
        "    jax.random.normal(subkeys[0], (400, 2)) + jnp.array([4, 0]),\n",
        "    jax.random.normal(subkeys[1], (200, 2)) + jnp.array([.5, 1]),\n",
        "    jax.random.normal(subkeys[2], (200, 2)) + jnp.array([-.5, -1]),\n",
        "])\n",
        "points"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[ 4.8073673 ,  0.69552976],\n",
              "             [ 2.8321383 ,  0.50319654],\n",
              "             [ 6.1076746 ,  0.8052495 ],\n",
              "             ...,\n",
              "             [ 1.2582104 , -1.5633142 ],\n",
              "             [ 1.091379  ,  0.04237545],\n",
              "             [ 0.5922272 ,  0.6641729 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR_l2qvFZtBu"
      },
      "source": [
        "#### Part 1: Initialize your centroids and distortions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUgGbn3aZejl"
      },
      "source": [
        "num_clusters = 4\n",
        "key, subkey = random.split(key)\n",
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV4f9OYTassx"
      },
      "source": [
        "### Part 2: Find new assignment and calculate new distortions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqggN5AV_Fmo"
      },
      "source": [
        "**Hint:** Use `jnp.argmin`, `jax.vmap` and `jnp.linalg.norm`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feY9NEhNb00r"
      },
      "source": [
        "def update_assignment(samples, centroids):\n",
        "    # TODO\n",
        "    return assignment, distortions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKT8JrtzgIpv"
      },
      "source": [
        "### Part 3: Find new centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCp2DxU_s2v"
      },
      "source": [
        "**Hint 1:** Use `jax.vmap`. \n",
        "\n",
        "**Hint 2:** What does this line do? \n",
        "\n",
        "```\n",
        "points_assignments[:, jnp.newaxis] == cluster_id[jnp.newaxis, jnp.newaxis]\n",
        "```\n",
        "**Hint 3:** You can use `jnp.where`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFV20PwyCDTS"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5io-lHDggOoR"
      },
      "source": [
        "### Part 4: Iterate!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8kSSWhYyvTZ"
      },
      "source": [
        "First wrap Part 2 and Part 3 in a function: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmKsabsLw6Po"
      },
      "source": [
        "def improve_centroids(values, k):\n",
        "    centroids, distortions, _ = values\n",
        "    # TODO\n",
        "    return new_centroids, new_distortions.mean(), jnp.mean(distortions)\n",
        "\n",
        "# let's test it\n",
        "improve_centroids((initial_centroids, initial_distortion, jnp.inf), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CepXVpiDA5ho"
      },
      "source": [
        "Use `jax.lax.while_loop`. It has [several benefits](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.while_loop.html) over the normal while loop.\n",
        "\n",
        "We use `partial` to provide the last argument of the above function (`k`) and allow the `values` tupple to be iterated on in a consistant manner using the `while_loop` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCdv3Y3D0yXD"
      },
      "source": [
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu41d9FdzTk2"
      },
      "source": [
        "thresh=1e-5\n",
        "centroids, distortion, _ = jax.lax.while_loop(\n",
        "        lambda values: (values[2] - values[1]) > thresh,\n",
        "        partial(improve_centroids, k = num_clusters),\n",
        "        (initial_centroids, initial_distortion, jnp.inf),\n",
        ")\n",
        "centroids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwPFnJq_y3ci"
      },
      "source": [
        "### Part 5: Visualize the clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Ym5pA3z4S3"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCYCZlzRzaWJ"
      },
      "source": [
        "# Perform the final assignment\n",
        "final_assignments, _ = update_assignment(points, centroids)\n",
        "\n",
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}